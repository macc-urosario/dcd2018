%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass{beamer}
%\documentclass[handout,ignorenonframetext]{beamer} 
%\documentclass[12pt,handout]{beamer}
%\documentclass{article} 
%\usepackage{beamerarticle}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
%\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

\usepackage{amsmath}
\usepackage[spanish]{babel}
\usepackage[latin1]{inputenc}
%\usepackage[utf8]{inputenc}
%\usepackage{verbatim}

\usepackage{amsthm}
\usepackage{amsfonts}


\usepackage{verbatim}
\usepackage{listings}
\usepackage{color}
\usepackage{enumitem}
\usepackage{soul}

\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{url}
\usepackage{hyperref}


%---------------------------------
% Commands
%-----------------
\newcommand*{\escape}[1]{\texttt{\textbackslash#1}}
\newcommand{\bs}{\boldsymbol}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}

\title[Aprendizaje No Supervisado: Clustering]{
Aprendizaje No Supervisado: Clustering} % The short title appears at the bottom of every slide, the full title is only on the title page

\author{Juan F.~Pérez} % Your name
\institute[Universidad del Rosario] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
Departamento MACC\\
Matemáticas Aplicadas y Ciencias de la Computación\\
Universidad del Rosario \\ % Your institution for the title page
\medskip
\textit{juanferna.perez@urosario.edu.co} % Your email address
}
%\date{Segundo Semestre de 2017} %\today} % Date, can be changed to a custom date
\date{2018} %\today} % Date, can be changed to a custom date

%\lstset{language=C,caption={Descriptive Caption Text},label=DescriptiveLabel}
\lstset{language=Python,showstringspaces=false}
\begin{document}

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}
\frametitle{Contenidos} % Table of contents slide, comment this block out to remove it
\tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
\end{frame}

%----------------------------------------------------------------------------------------
%	PRESENTATION SLIDES
%----------------------------------------------------------------------------------------

%------------------------------------------------

%------------------------------------------------

\section{Introducción}

\begin{frame}
\frametitle{Introducción}
\begin{itemize}\itemsep3pt
	\item Búsqueda de patrones
	\item Estudio de fenómenos físicos
	\item Reconocimiento de patrones 
	\item Descubrimiento automático de regularidades
	\item Algoritmos computacionales
\end{itemize}
\end{frame}

%\begin{frame}
%\frametitle{Ejemplo}
%Reconocimiento de dígitos 
%\begin{center}
%\includegraphics[width=0.6\textwidth]{figs/mnist_digits}
%\end{center}
%Tomado de ``Training Invariant Support Vector Machines'', Machine Learning, 46, 161-190, 2002. 
%%Tomado de ``Training Invariant Support Vector Machines'', Machine Learning, 46, 161–190, 2002. 
%
%\end{frame}

\begin{frame}[fragile]
\frametitle{Algunos problemas de aprendizaje}
\begin{itemize}\itemsep5pt
	\item<1-> Datos de entrenamiento contienen las características $x_i$ como las categorías/etiquetas $t_i$
	\item[]<2-> 
	\begin{center}
	\emph{Aprendizaje supervisado}	
	\end{center}
	%\item 
	\item<3-> Resultado es una o varias variables continuas (no un número finito de categorías) 
	\item[]<4-> 
	\begin{center}
		\emph{Regresión}
	\end{center}
\end{itemize}
%\end{block}
\end{frame}

\begin{frame}[fragile]
\frametitle{Algunos problemas de aprendizaje}
\begin{itemize}\itemsep5pt
	\item<1-> Datos de entrenamiento contienen las características $x_i$ como las categorías/etiquetas $t_i$
	\item[]<2-> 
	\begin{center}
	\emph{Aprendizaje supervisado}	
	\end{center}
	%\item 
	\item<3-> Resultado es una categoría (de un número finito de posibles categorías) 
	\item[]<4-> 
	\begin{center}
		\emph{Clasificación}
	\end{center}
\end{itemize}
%\end{block}
\end{frame}

\begin{frame}[fragile]
\frametitle{Algunos problemas de aprendizaje}
\begin{itemize}\itemsep5pt
	\item<1-> Datos de entrenamiento contienen las características $x_i$ pero NO las categorías/etiquetas $t_i$
	\item[]<2-> 
	\begin{center}
	\emph{Aprendizaje no supervisado}	
	\end{center}
	%\item 
	\item<3-> Objetivo es descubrir grupos similares
	\item[]<4-> 
	\begin{center}
		\emph{Clustering (análisis de conglomerados)}
	\end{center}
\end{itemize}
%\end{block}
\end{frame}

\begin{frame}[fragile]
\frametitle{Algunos problemas de aprendizaje}
\begin{itemize}\itemsep5pt
	\item<1-> Datos de entrenamiento contienen las características $x_i$ pero NO las categorías/etiquetas $t_i$
	\item[]<2-> 
	\begin{center}
	\emph{Aprendizaje no supervisado}	
	\end{center}
	%\item 
	\item<3-> Objetivo es determinar la distribución de los datos en el espacio de entrada
	\item[]<4-> 
	\begin{center}
		\emph{Estimación de densidades}
	\end{center}
\end{itemize}
%\end{block}
\end{frame}





\section{Clustering/Análisis de Conglomerados} 
\begin{frame}[fragile]
\frametitle{Clustering/Análisis de Conglomerados}
Objetivo:
\begin{itemize}\itemsep3pt
	\item<1-> Dadas $n$ observaciones cada una descrita como un vector $x$ de dimensión $D$ (características)
	\item<2-> Descubrir grupos de observaciones similares 
	\item<3-> Grupo $=$ cluster $=$ conglomerado 
	\item<4-> \textbf{NO} hay etiquetas $\rightarrow$ no sabemos si efectivamente hay clusters o cuántos hay
	\item<5-> $\rightarrow$ \emph{diferente} al problema de clasificación
\end{itemize}

\end{frame}




\begin{frame}[fragile]
\frametitle{Ejemplos}
\begin{itemize}\itemsep3pt
	\item<1-> Se toman muestras de un tejido canceroso de $n$ pacientes 
	\item<2-> Para cada muestra se obtienen $D$ descriptores (características): medidas físicas, químicas, imágenes 
	\item<3-> Se busca identificar casos/muestras similares 
	\item<4-> Podrían reflejar estados similares de avance de la enfermedad, respuesta similar a tratamiento, tipos diferentes de enfermedad/paciente
	\item<5-> No se sabe \textit{a priori} pero se quiere explorar
\end{itemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{Ejemplos}
\begin{itemize}\itemsep3pt
	\item<1-> Se tiene información de $n$ clientes 
	\item<2-> Para cada cliente se obtienen $D$ descriptores (características): hábitos de compra, datos socio-demográficos
	\item<3-> Se busca identificar clientes similares 
	\item<4-> Podrían reflejar potenciales clientes de nuevos productos, interés en ofertas de cierto tipo, capacidad/deseo de compra de ciertos artículos
	\item<5-> No se sabe \textit{a priori} pero se quiere explorar
	\item<6-> (\emph{Segmentación del mercado}) 
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Resultado}
\begin{itemize}\itemsep3pt
	\item<1-> Clusters de observaciones similares 
	\item<2-> Cada cluster puede reflejar un conjunto de interés a analizar 
	\item<3-> Reducción o simplificación de información
	\item<4-> Simplificar o posibilitar el análisis de grandes cantidades de información multi-dimensional
\end{itemize}
\end{frame}


\section{K-means} 
\begin{frame}[fragile]
\frametitle{K-means}
\begin{itemize}\itemsep3pt
	\item<1-> Agrupar observaciones en $K$ clusters
	\item<2-> Para cada observación se determina a cuál de los clusters pertenece (solo uno)
	\item<3-> Clusters $C_1,\dots,C_K$
	\item<4-> Toda observación pertenece a un solo cluster
\end{itemize}

\end{frame}

%\[y(x) = w^{T}x + w_0\]
\begin{frame}[fragile]
\frametitle{K-means}
\begin{itemize}\itemsep3pt
	\item<1-> Visión 1:
	\begin{itemize}\itemsep3pt
		\item<2-> Observaciones en un mismo cluster deben ser parecidas entre sí 
		\item<3-> Observaciones en clusters diferentes deben ser relativamente diferentes 
	\end{itemize}
	\item<4-> Visión 2:
	\begin{itemize}\itemsep3pt
		\item<5-> Una observación debe ser más parecida a otras observaciones en el mismo cluster que a observaciones en otros clusters
	\end{itemize}
	\item<6-> Visión 3:
	\begin{itemize}\itemsep3pt
		\item<7-> Minimizar la variabilidad al interior de cada cluster conformado 
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{K-means}
\begin{itemize}\itemsep3pt
	\item<1-> Minimizar la variabilidad al interior de cada cluster conformado 
	\item<2-> Sea $V_k$ una medida de la variabilidad en el cluster $k$ 
	\item<3-> Objetivo de K-means:
	\[
	\min \sum_{k=1}^{K} V_k
	\]
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{K-means}
\begin{itemize}\itemsep3pt
	\item<1-> Determinar $V_k$ una medidad de la variabilidad en el cluster $k$ 
	\item<2-> Distancia (euclideana) entre todos los puntos del cluster
	\item<3-> Suponiendo una sola característica ($x_i$ es un número)
	\[
	V_k = \frac{1}{|C_k|}\sum_{i,j\in C_k} (x_{i} - x_{j})^2
	\]
	\item<4-> Caso general con $D$ características ($x_i$ es un vector con entradas $x_{i,d}$):
	\[
	V_k = \frac{1}{|C_k|}\sum_{i,j\in C_k} \sum_{d=1}^D(x_{i,d} - x_{j,d})^2
	\]
\end{itemize}
\end{frame}


\begin{frame}[fragile]
\frametitle{K-means}
\begin{itemize}\itemsep3pt
	\item<1-> Objetivo de K-means:
	\[
	\min \sum_{k=1}^{K} \frac{1}{|C_k|} \sum_{i,j\in C_k} \sum_{d=1}^D(x_{i,d} - x_{j,d})^2
	\]
	\item<2-> Problema de optimización 
	\item<3-> $K^n$ formas de asignar $n$ observaciones a $K$ clusters
	\item<4-> ¿Cómo resolverlo?
\end{itemize}
\end{frame}


\begin{frame}[fragile]
\frametitle{K-means: Algoritmo}
\begin{itemize}\itemsep3pt
	\item<1-> Algoritmo sencillo que alcanza encuentra una muy buena solución
	\item<2-> Algoritmo eficiente: ejecución veloz y escalable a muchos datos
	\item<3-> No garantiza que se encuentre la mejor solución (óptimo)
	\item<4-> El centroide $y_k$ del cluster $k$: 
	\[
	y_k = \frac{1}{|C_k|} \sum_{i\in C_k} \sum_{d=1}^D x_{i,d} 
	\]
	\item<5-> Centroide $y_k$: punto medio del cluster $k$
\end{itemize}
\end{frame}


\begin{frame}[fragile]
\frametitle{K-means: Algoritmo}
\begin{enumerate}\itemsep3pt
	\item Seleccione $K$ centroides al azar, uno para cada cluster
	\item Hasta que los centroides no cambien más, \emph{repita}: 
	\begin{enumerate}\itemsep3pt
		\item Asigne cada observación al cluster más cercano 
		\item Para cada cluster, re-calcule el centroide 
	\end{enumerate}
\end{enumerate}
\end{frame}


\section{K-Means en Scikit-Learn}
\begin{frame}[fragile]
\frametitle{K-Means en Scikit-Learn}
\begin{lstlisting}
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()

from sklearn.datasets.samples_generator 
          import make_blobs
X, y = make_blobs(n_samples=300, centers=4, 
          cluster_std=0.6, random_state=1)
plt.scatter(X[:,0], X[:,1], s=50)
\end{lstlisting}
\end{frame}


\begin{frame}[fragile]
\frametitle{K-Means en Scikit-Learn}
\begin{lstlisting}
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=4)
y_kmeans = kmeans.fit_predict(X)

plt.scatter(X[:,0], X[:,1], c=y_kmeans, s=50, 
          cmap='viridis')
centros = kmeans.cluster_centers_
print(centros)
plt.scatter(centros[:,0], centros[:,1], 
          c='black', s=200, alpha=0.5)
\end{lstlisting}
\end{frame}


\begin{frame}[fragile]
\frametitle{K-Means en Scikit-Learn: Evolución}
\begin{lstlisting}
from sklearn.datasets.samples_generator 
          import make_blobs

X, y = make_blobs(n_samples=300, centers=4, 
          cluster_std=0.6, random_state=1)
plt.figure()    
plt.scatter(X[:,0], X[:,1], c='green', s=50)

cents, etiqs = kmeans_paso_a_paso(X,4,3,100)
\end{lstlisting}

\begin{itemize}
	\item<2> Descargar la función \texttt{kmeans\_paso\_a\_paso} del repositorio
\end{itemize}

\end{frame}




\begin{frame}[fragile]
\frametitle{K-Means para reconocer dígitos similares}
\begin{lstlisting}
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()
import numpy as np
from sklearn.datasets import load_digits
from sklearn.cluster import KMeans
\end{lstlisting}
\end{frame}


\begin{frame}[fragile]
\frametitle{K-Means para reconocer dígitos similares}
\begin{lstlisting}
digits = load_digits()
print(digits.data.shape)

kmeans = KMeans(n_clusters=10, random_state=0)
clusters = kmeans.fit_predict(digits.data)
kmeans.cluster_centers_.shape
\end{lstlisting}
\end{frame}

\begin{frame}[fragile]
\frametitle{K-Means para reconocer dígitos similares}
\begin{lstlisting}
fig, ax = plt.subplots(2, 5, figsize=(8, 3))
centers = kmeans.cluster_centers_.reshape(10, 8, 8)
for axi, center in zip(ax.flat, centers):
    axi.set(xticks=[], yticks=[])
    axi.imshow(center, interpolation='nearest', 
                cmap=plt.cm.binary)

\end{lstlisting}
\end{frame}

\begin{frame}[fragile]
\frametitle{K-Means para reconocer dígitos similares}
\begin{lstlisting}
from scipy.stats import mode
etiqs = np.zeros_like(clusters)
for i in range(10):
    mask = (clusters == i)
    etiqs[mask] = mode(digits.target[mask])[0]    
\end{lstlisting}
\end{frame}


\begin{frame}[fragile]
\frametitle{K-Means para reconocer dígitos similares}
\begin{lstlisting}
from sklearn.metrics import confusion_matrix
mat = confusion_matrix(digits.target, etiqs)
plt.figure()
sns.heatmap(mat.T, square=True, annot=True, fmt='d', 
            cbar=False,
            xticklabels=digits.target_names,
            yticklabels=digits.target_names)
plt.xlabel('etiqueta observada')
plt.ylabel('etiqueta predicha')
\end{lstlisting}
\end{frame}


\begin{frame}[fragile]
\frametitle{K-Means para reconocer dígitos similares}
\begin{lstlisting}
from sklearn.metrics import accuracy_score
print("precisión:", 
    accuracy_score(digits.target, etiqs))
\end{lstlisting}
\end{frame}


\section{K-Means para comprimir imágenes}
\begin{frame}[fragile]
\frametitle{K-Means para comprimir imágenes}
\begin{lstlisting}
from sklearn.datasets import load_sample_image
import matplotlib.pyplot as plt
import numpy as np

china = load_sample_image("china.jpg")
ax = plt.axes(xticks=[], yticks=[])
ax.imshow(china)
\end{lstlisting}
\end{frame}

\begin{frame}[fragile]
\frametitle{K-Means para comprimir imágenes}
\begin{lstlisting}
print(china.shape)
data = china / 255.0 # escala 0 a 1 
data = data.reshape(427 * 640, 3)
print(data.shape)
\end{lstlisting}
\end{frame}


\begin{frame}[fragile]
\frametitle{K-Means para comprimir imágenes}
\begin{lstlisting}
def plot_pixels(data, title, colors=None, N=10000):
    if colors is None:
        colors = data
        
    # seleccionar sub-conjunto aleatoriamente
    rng = np.random.RandomState(0)
    i = rng.permutation(data.shape[0])[:N]
    colors = colors[i]
\end{lstlisting}
\end{frame}


\begin{frame}[fragile]
\frametitle{K-Means para comprimir imágenes}
\begin{lstlisting}
    R, G, B = data[i].T
    fig, ax = plt.subplots(1, 2, figsize=(16, 6))
    ax[0].scatter(R, G, color=colors, marker='.')
    ax[0].set(xlabel='Red', ylabel='Green', 
		    xlim=(0, 1), ylim=(0, 1))
    ax[1].scatter(R, B, color=colors, marker='.')
    ax[1].set(xlabel='Red', ylabel='Blue', 
		    xlim=(0, 1), ylim=(0, 1))
    fig.suptitle(title, size=20);
\end{lstlisting}
\end{frame}



\begin{frame}[fragile]
\frametitle{K-Means para comprimir imágenes}
\begin{lstlisting}
plot_pixels(data, title='Espacio de Colores inicial: 
          16 millones de colores posibles')
\end{lstlisting}
\end{frame}


\begin{frame}[fragile]
\frametitle{K-Means para comprimir imágenes}
\begin{lstlisting}
from sklearn.cluster import MiniBatchKMeans
kmeans = MiniBatchKMeans(16)
kmeans.fit(data)
new_colors = kmeans.cluster_centers_[
        kmeans.predict(data)]
plot_pixels(data, colors=new_colors, 
        title="Espacio reducido: 16 colores")
\end{lstlisting}
\end{frame}


\begin{frame}[fragile]
\frametitle{K-Means para comprimir imágenes}
\begin{lstlisting}
china_recolored = new_colors.reshape(china.shape)
plt.figure()
fig, ax = plt.subplots(1, 2, figsize=(16, 6), 
          subplot_kw=dict(xticks=[], yticks=[]))
\end{lstlisting}
\end{frame}

\begin{frame}[fragile]
\frametitle{K-Means para comprimir imágenes}
\begin{lstlisting}
fig.subplots_adjust(wspace=0.05)
ax[0].imshow(china)
ax[0].set_title('Imagen original', size=16)
ax[1].imshow(china_recolored)
ax[1].set_title('Imagen en 16 colores', size=16)
\end{lstlisting}
\end{frame}


\section{Datos sin fronteras lineales}
\begin{frame}[fragile]
\frametitle{Datos sin fronteras lineales}
\begin{lstlisting}
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()
import numpy as np

from sklearn.datasets import make_moons
X, y = make_moons(n_samples=300, noise = 0.05, 
          random_state=1)
plt.scatter(X[:,0], X[:,1], s=50)
\end{lstlisting}
\end{frame}


\begin{frame}[fragile]
\frametitle{Datos sin fronteras lineales}
\begin{lstlisting}
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=2)
kmeans.fit(X)
y_kmeans = kmeans.predict(X)

plt.scatter(X[:,0], X[:,1], c=y_kmeans, s=50, 
          cmap='viridis')
centers = kmeans.cluster_centers_
print(centers)
plt.scatter(centers[:,0], centers[:,1], c='black', 
          s=200, alpha=0.5  )
\end{lstlisting}
\end{frame}


\begin{frame}[fragile]
\frametitle{Datos sin fronteras lineales}
\begin{lstlisting}
from sklearn.cluster import SpectralClustering
modelo = SpectralClustering(n_clusters=2, 
          affinity='nearest_neighbors', 
          assign_labels='kmeans')
y_modelo = modelo.fit_predict(X)

plt.figure()
plt.scatter(X[:,0], X[:,1], c=y_modelo, s=50, 
          cmap='viridis')
\end{lstlisting}
\end{frame}


\section{Clustering Jerárquico}
\begin{frame}[fragile]
\frametitle{Clustering Jerárquico}
Dendogramas
\begin{lstlisting}
import numpy as np

X = np.array([[5,3],  
    [10,15],
    [15,12],
    [24,10],
    [30,30],
    [85,70],
    [71,80],
    [60,78],
    [70,55],
    [80,91],])
\end{lstlisting}
\end{frame}


\begin{frame}[fragile]
\frametitle{Clustering Jerárquico}
Dendogramas y Clustering Jerárquico
\begin{lstlisting}
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()
import numpy as np

from sklearn.datasets.samples_generator import make_blobs
X, y = make_blobs(n_samples=300, centers=4, 
        cluster_std=0.6, random_state=1)
plt.scatter(X[:,0], X[:,1], s=50)
\end{lstlisting}
\end{frame}


\begin{frame}[fragile]
\frametitle{Clustering Jerárquico}
\begin{lstlisting}
import scipy.cluster.hierarchy as shc
plt.figure()
plt.title(u"Dendograma de característica 1")
dend = shc.dendrogram(shc.linkage(X, method='ward') )
\end{lstlisting}
\end{frame}


\begin{frame}[fragile]
\frametitle{Clustering Jerárquico}
\begin{lstlisting}
from sklearn.cluster import AgglomerativeClustering
modelo = AgglomerativeClustering(n_clusters=4, 
        affinity='euclidean',linkage='ward')
y_modelo = modelo.fit_predict(X)

plt.figure()
plt.scatter(X[:,0], X[:,1], c=y_modelo, s=50, 
        cmap='viridis')
\end{lstlisting}
\end{frame}
%------------------------------------------------

%\begin{frame}
%\frametitle{Multiple Columns}
%\begin{columns}[c] % The "c" option specifies centered vertical alignment while the "t" option is used for top vertical alignment
%
%\column{.45\textwidth} % Left column and width
%\textbf{Heading}
%\begin{enumerate}
%\item Statement
%\item Explanation
%\item Example
%\end{enumerate}
%
%\column{.5\textwidth} % Right column and width
%Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer lectus nisl, ultricies in feugiat rutrum, porttitor sit amet augue. Aliquam ut tortor mauris. Sed volutpat ante purus, quis accumsan dolor.
%
%\end{columns}
%\end{frame}

%------------------------------------------------
%\section{Second Section}
%------------------------------------------------

%\begin{frame}
%\frametitle{Table}
%\begin{table}
%\begin{tabular}{l l l}
%\toprule
%\textbf{Treatments} & \textbf{Response 1} & \textbf{Response 2}\\
%\midrule
%Treatment 1 & 0.0003262 & 0.562 \\
%Treatment 2 & 0.0015681 & 0.910 \\
%Treatment 3 & 0.0009271 & 0.296 \\
%\bottomrule
%\end{tabular}
%\caption{Table caption}
%\end{table}
%\end{frame}

%------------------------------------------------

%\begin{frame}
%\frametitle{Theorem}
%\begin{theorem}[Mass--energy equivalence]
%$E = mc^2$
%\end{theorem}
%\end{frame}

%------------------------------------------------

%\begin{frame}[fragile] % Need to use the fragile option when verbatim is used in the slide
%\frametitle{Verbatim}
%\begin{example}[Theorem Slide Code]
%\begin{verbatim}
%\begin{frame}
%\frametitle{Theorem}
%\begin{theorem}[Mass--energy equivalence]
%$E = mc^2$
%\end{theorem}
%\end{frame}\end{verbatim}
%\end{example}
%\end{frame}

%\begin{frame}[fragile] % Need to use the fragile option when verbatim is used in the slide
%\begin{lstlisting}
%a=5;
%b=3;
%c=a+b;
%print c;
%\end{lstlisting}
%\end{frame}

%------------------------------------------------

%\begin{frame}
%\frametitle{Figure}
%Uncomment the code on this slide to include your own image from the same directory as the template .TeX file.
%%\begin{figure}
%%\includegraphics[width=0.8\linewidth]{test}
%%\end{figure}
%\end{frame}

%------------------------------------------------

%\begin{frame}[fragile] % Need to use the fragile option when verbatim is used in the slide
%\frametitle{Citation}
%An example of the \verb|\cite| command to cite within the presentation:\\~
%
%This statement requires citation \cite{p1}.
%\end{frame}

%------------------------------------------------

%\begin{frame}
%\frametitle{References}
%\footnotesize{
%\begin{thebibliography}{99} % Beamer does not support BibTeX so references must be inserted manually as below
%\bibitem[Smith, 2012]{p1} John Smith (2012)
%\newblock Title of the publication
%\newblock \emph{Journal Name} 12(3), 45 -- 678.
%\end{thebibliography}
%}
%\end{frame}

%------------------------------------------------

%\begin{frame}
%\Huge{\centerline{The End}}
%\end{frame}

%----------------------------------------------------------------------------------------

\end{document} 